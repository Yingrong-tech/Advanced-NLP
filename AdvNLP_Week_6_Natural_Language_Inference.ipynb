{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Developing a Content Moderation System by Natural Language Inference\n",
        "\n",
        "The code sets up a Natural Language Inference (NLI) system using the DistilBERT model for content moderation. The code loads the MultiNLI dataset and use it to fine-tunes a DistilBERT model to classify input relationships as entailment, contradiction, or neutral. The fine-tuned model can then evaluate user posts (hypothesis) against platform terms of service (premise) to detect policy violations. By using the fine-tuned model, the system can automatically flag inappropriate or non-compliant content."
      ],
      "metadata": {
        "id": "Er4rRSXfKYQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Install necessary packages and import libraries"
      ],
      "metadata": {
        "id": "P5yC5oWgLKPo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijk2304b_iYp",
        "outputId": "4e96aed9-155c-4759-9ea7-1151e6c2c4b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install the required libraries.\n",
        "# 'transformers' for access to pre-trained models like DistilBERT and related utilities.\n",
        "# 'datasets' for easy loading and handling of datasets like MultiNLI.\n",
        "# 'torch' for building and running deep learning models using PyTorch.\n",
        "!pip install transformers datasets torch\n",
        "\n",
        "# Import specific classes and functions from the installed libraries.\n",
        "# 'DistilBertTokenizer' for tokenizing text data using the DistilBERT vocabulary.\n",
        "# 'DistilBertForSequenceClassification' is the DistilBERT model with a classification head on top, suitable for tasks like NLI.\n",
        "# 'Trainer' is a utility class from the transformers library to simplify model training and evaluation.\n",
        "# 'TrainingArguments' is used to define the hyperparameters and configuration for the Trainer.\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Import the 'load_dataset' function from the datasets library to easily load benchmark datasets.\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Import the 'torch' library, the main deep learning framework used.\n",
        "import torch\n",
        "\n",
        "# Import the 'warnings' module to manage warnings that might be generated during execution.\n",
        "import warnings\n",
        "\n",
        "# Import the 'os' module to interact with the operating system, such as setting environment variables.\n",
        "import os\n",
        "\n",
        "# Set the environment variable 'WANDB_DISABLED' to \"true\".\n",
        "# This prevents the transformers Trainer from automatically initializing and reporting to Weights & Biases (wandb),\n",
        "# which avoids the need for a wandb API key prompt.\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Device setup and dataset loading"
      ],
      "metadata": {
        "id": "krUpXFHmLOl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect if GPU is available and set the device (either GPU or CPU) accordingly for faster computation.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the MultiNLI dataset for Natural Language Inference (NLI).\n",
        "mnli_dataset = load_dataset(\"multi_nli\")\n",
        "\n",
        "# Initialize the DistilBERT tokenizer, a smaller and faster version of BERT, which will handle input tokenization.\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMrw-acJK6Xj",
        "outputId": "4738a828-fd2f-4e3f-84a4-967e13904c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Tokenization and dataset preprocessing"
      ],
      "metadata": {
        "id": "CYmb5nBuLSfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a tokenization function to process premise and hypothesis pairs. We apply truncation and padding\n",
        "# to ensure each input has the same length for efficient model processing.\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example['premise'], example['hypothesis'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "# Suppress warnings related to token truncation for a cleaner output.\n",
        "warnings.filterwarnings(\"ignore\", message=\"Be aware, overflowing tokens are not returned*\")\n",
        "\n",
        "# Apply the tokenization function to the dataset in batches using multiple processors for faster processing.\n",
        "# Adjust 'num_proc' based on the number of available CPU cores (e.g., num_proc=4).\n",
        "tokenized_mnli = mnli_dataset.map(tokenize_function, batched=True, num_proc=4)\n",
        "\n",
        "# Remove unnecessary columns (premise and hypothesis) from the dataset and set the format for PyTorch.\n",
        "tokenized_mnli = tokenized_mnli.remove_columns(['premise', 'hypothesis'])\n",
        "tokenized_mnli.set_format(\"torch\")\n"
      ],
      "metadata": {
        "id": "Vpe7CAOkK_Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 4: Model setup and training configuration"
      ],
      "metadata": {
        "id": "yJJROneeLV4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained DistilBERT model, which includes a classification head with 3 output labels (entailment, contradiction, neutral).\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU).\n",
        "model.to(device)\n",
        "\n",
        "# Define training arguments for fine-tuning the model, including batch sizes, learning rate, number of epochs, and weight decay.\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # Directory for saving model checkpoints and results.\n",
        "    eval_strategy=\"epoch\",           # Evaluate the model at the end of each epoch.\n",
        "    learning_rate=2e-5,              # Learning rate for the optimizer.\n",
        "    per_device_train_batch_size=16,  # Batch size for training on each device.\n",
        "    per_device_eval_batch_size=16,   # Batch size for evaluation on each device.\n",
        "    num_train_epochs=3,              # Number of training epochs.\n",
        "    weight_decay=0.01,               # Weight decay for regularization.\n",
        "    report_to=None,                  # Disable reporting.\n",
        ")\n",
        "\n",
        "# Initialize the Trainer with the model, training arguments, and datasets for training and evaluation.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_mnli[\"train\"].shuffle(seed=42).select(range(2000)),  # Using a small subset of the training data for a quick run.\n",
        "    eval_dataset=tokenized_mnli[\"validation_matched\"].select(range(500)),        # Using a small subset of the validation data for quick evaluation.\n",
        ")\n",
        "\n",
        "# Fine-tune the model using the Trainer API.\n",
        "trainer.train()\n",
        "\n",
        "# Print evaluation results after fine-tuning (this will be added after evaluating the model).\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation Results: {eval_results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "DtJiOckDLCSd",
        "outputId": "715a8467-840b-4752-e8e9-e712570477ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [375/375 01:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.075831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.971679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.937023</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'eval_loss': 0.9370229244232178, 'eval_runtime': 1.7378, 'eval_samples_per_second': 287.717, 'eval_steps_per_second': 18.414, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 5: Inference and testing the model"
      ],
      "metadata": {
        "id": "KcMYMROeLZJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function for making predictions using the fine-tuned model.\n",
        "def predict_nli(premise, hypothesis):\n",
        "    # Tokenize the premise and hypothesis inputs.\n",
        "    inputs = tokenizer(premise, hypothesis, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    # Move the tokenized inputs to the same device as the model (GPU or CPU).\n",
        "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "    # Put the model in evaluation mode and perform inference without gradient calculation.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Get the predicted class label (entailment, neutral, contradiction).\n",
        "    predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
        "    label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
        "    return label_map[predicted_class_id]\n",
        "\n",
        "# Example inference related to climate change misinformation\n",
        "premise = \"The platform prohibits the spread of scientifically unverified claims, particularly those that can mislead people about critical global issues like climate change.\"\n",
        "hypothesis = \"Climate change is a hoax, and global warming is just a natural cycle that has nothing to do with human activity.\"\n",
        "result = predict_nli(premise, hypothesis)\n",
        "print(f\"Prediction: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuUnsyZILERn",
        "outputId": "59b8a91f-1a09-4e4d-fbae-41521bdaf2cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: contradiction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5e7a35d"
      },
      "source": [
        "## Section 6: Experiment with your own examples\n",
        "\n",
        "Now that you have a trained model, feel free to experiment with your own premise and hypothesis pairs using the `predict_nli` function. You can test different scenarios and see how the model classifies the relationship between the premise and the hypothesis.\n",
        "\n",
        "From your own examples find one result that you disagree with. Share your findings and examples with your classmates!"
      ]
    }
  ]
}